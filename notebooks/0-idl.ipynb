{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Deep Learning\n",
    "\n",
    "### Linear layer and single-layer perceptron\n",
    "\n",
    "Given the input vector $x$, a _linear layer_ effectively calculates the output\n",
    "vector $\\hat{y}$ by the following equation, where $W$ and $b$ are the layer's\n",
    "weight matrix and bias vector, respectively:\n",
    "\n",
    "$$ \\hat{y} = W \\cdot x + b $$\n",
    "\n",
    "Note that linear layer is **not** a linear transformation, despite its name. We\n",
    "could easily create a linear layer by the following code snippet. Here, we\n",
    "assume $\\hat{y}$ a 100-dimension vector and $x$ a 300-dimension vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "W = torch.randn((100, 300))\n",
    "x = torch.randn(300)\n",
    "b = torch.randn(100)\n",
    "\n",
    "y_hat = W @ x + b\n",
    "y_hat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, right, it's meaningless. The code just creates a random matrix and two\n",
    "random vectors, then wastes some electricity by matrix multiplying and adding\n",
    "them. Still, we've got the point:\n",
    "\n",
    "- In pytorch, matrices and vectors could be multiplied with the operator `@`.\n",
    "  Additionally, function `torch.mm()` could do the same job.\n",
    "- We can get the \"shape\" of a vector (a matrix, or, as in PyTorch docs, a\n",
    "  _tensor_) with the property `x.shape`. Additionally, method `x.size()` could\n",
    "  do the same job.\n",
    "\n",
    "Of course, writing `W`, `b` every time using such layer is a bad idea, and as\n",
    "expected, PyTorch has a solution to this problem: a\n",
    "[`Linear` class](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "# Note that the order of the dimensions are reversed\n",
    "linear = nn.Linear(300, 100)\n",
    "\n",
    "# Ignore the following two lines, this is just to make sure that\n",
    "# the parameters are the same with the previous code\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "linear.weight = Parameter(W)\n",
    "linear.bias = Parameter(b)\n",
    "\n",
    "# Calculate the output\n",
    "y_hat_nn = linear(x)\n",
    "\n",
    "# Check for correctness\n",
    "torch.allclose(y_hat, y_hat_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wondering why not just test equivalence? It's because of the\n",
    "[tradeoff between range and precision](https://en.wikipedia.org/wiki/Floating-point_arithmetic).\n",
    "Most floating point arithmetics are not exact; thus, testing the equality of two\n",
    "floating point numbers is dangerous in most cases. This subject is beyond our\n",
    "scope, so we'll just go on. However, it is an interesting subject, so\n",
    "consider reading the linked Wikipedia article. It really helps debug some\n",
    "related problems.\n",
    "\n",
    "Now, time to introduce an _activation function_. This is required because\n",
    "multiple linear layers is just another single linear layer plus excessive math:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{y} &= W_2 \\cdot (W_1 \\cdot x + b_1) + b_2 \\\\\n",
    "\t\t&= W_2 \\cdot W_1 \\cdot x + W_2 \\cdot b_1 + b_2 \\\\\n",
    "\t\t&= W' \\cdot x + b'\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Thank you, Mr. Algebra, Linear. Want to waste a huge amount of electricity and\n",
    "time calculating a single matrix multiplication followed by a vector addition?\n",
    "Try constructing a 100-layered, _deep_ neural network using only linear layers.\n",
    "Great idea, isn't it?\n",
    "\n",
    "So we need something nonlinear. A very simple example is a Heaviside step\n",
    "function:\n",
    "\n",
    "$$\n",
    " f(x) = \\begin{cases}\n",
    "\t1, & \\text{$x \\gt 0$} \\\\\n",
    "\t0, & \\text{otherwise}\n",
    " \\end{cases}\n",
    "$$\n",
    "\n",
    "A linear layer followed by the step function is commonly referred to as a\n",
    "single-layer perceptron or just a\n",
    "[_perceptron_](https://en.wikipedia.org/wiki/Perceptron). It is a binary\n",
    "classifier, thus could solve some problems.\n",
    "\n",
    "However, a single layer perceptron is a\n",
    "[linear classifier](https://en.wikipedia.org/wiki/Linear_classifier), i.e., a\n",
    "classifier that makes decisions based on a linear function acting on features.\n",
    "It, however, is not quite useful out-of-the-box since some of the problems are\n",
    "not solvable with them.\n",
    "\n",
    "As usual in math, some \"some of the problems\" are trivial. One example is the following\n",
    "function (where $x$ and $y$ are both $0$ or $1$):\n",
    "\n",
    "$$\n",
    " f(x, y) = \\begin{cases}\n",
    "\t0, & x = y \\\\\n",
    "\t1, & \\text{otherwise}\n",
    " \\end{cases}\n",
    "$$\n",
    "\n",
    "Yes, it's an XOR (exclusive or). Surprisingly, emulating the behavior of an XOR\n",
    "function with a 2-dimensional perceptron is impossible. This problem was so\n",
    "shocking that it even received a dedicated name: an _XOR problem_ (quite\n",
    "intuitive, isn't it?).\n",
    "\n",
    "Let's plot the XOR function on a graph, then verify why a linear classifier\n",
    "could not solve such problems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Ignore below code block, just for formatting\n",
    "sns.set_theme(style=\"ticks\")\n",
    "ax = plt.axes(aspect=\"equal\",\n",
    "              xlim=(-0.3, 1.5),\n",
    "              ylim=(-0.3, 1.5),\n",
    "              xticks=[0, 1],\n",
    "              yticks=[0, 1])\n",
    "ax.grid(True)\n",
    "sns.despine(ax=ax, offset=0)\n",
    "ax.spines[\"left\"].set_position((\"data\", 0))\n",
    "ax.spines[\"bottom\"].set_position((\"data\", 0))\n",
    "\n",
    "# Introduce variables\n",
    "x = np.array([0, 1, 0, 1])\n",
    "y = np.array([0, 0, 1, 1])\n",
    "xor = (x ^ y).astype(np.bool_)\n",
    "\n",
    "# Plot true as 'o'\n",
    "ax.scatter(x[xor], y[xor], s=100, facecolors=\"none\", edgecolors=\"k\")\n",
    "# Plot false as 'x'\n",
    "ax.scatter(x[~xor], y[~xor], s=100, c=\"k\", marker=\"x\")\n",
    "# Show\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep learning and multilayer perceptrons\n",
    "\n",
    "So we need multiple layers. This is the main idea of _deep learning_: emulating\n",
    "a nonlinear function with linear operations on the feature vectors plus a\n",
    "nonlinear activation function. The advantages of such method are as follows:\n",
    "\n",
    "1. **Relatively cheaper computational cost.** The cost would be even lowered if\n",
    "   hardware accelerators (a.k.a. GPUs) take part.\n",
    "2. **Less subject to _a priori_ knowledge to the problem.** There is no need to\n",
    "   figure out the governing equation of the system of interest analytically.\n",
    "   Yet, this is also true of the other numerical approximation algorithms.\n",
    "3. **Easily scalable.** This is the main difference compared to the other\n",
    "   machine learning algorithms. Millions of, or even billions of, parameters are\n",
    "   not rare in deep learning models. Thus, complex problems related to a huge\n",
    "   dataset is the main target for deep learning models.\n",
    "\n",
    "### GPU training\n",
    "\n",
    "#### Selecting GPUs\n",
    "\n",
    "If multiple GPUs are connected to the machine, PyTorch automatically selects the\n",
    "GPU with index `0`. If such behavior is not desirable, you may set the\n",
    "environment variable `$CUDA_VISIBLE_DEVICES` to the index of the GPU to use.\n",
    "\n",
    "Before selection, check others' GPU usage with the command `nvidia-smi`. With an\n",
    "exclamation point at the beginning of the line, we could interleave shell\n",
    "commands between python codes in jupyter notebooks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jan  9 17:34:07 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.74       Driver Version: 470.74       CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:02:00.0 Off |                  N/A |\n",
      "| 21%   46C    P0    52W / 215W |      0MiB /  7981MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  Off  | 00000000:81:00.0 Off |                  N/A |\n",
      "| 37%   51C    P0    49W / 215W |      0MiB /  7982MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then set the env variable to the zero-based index of the CUDA device to use.\n",
    "Note that this should be done in the command line or, at least, before importing\n",
    "the `torch` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to select the GPU is to explicitly set the device ID like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:1\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyTorch and GPU\n",
    "\n",
    "PyTorch can handle both CPU and GPU tensors. However, computation between CPU\n",
    "and GPU tensors is forbidden; all tensors should be on the same device. The\n",
    "default device in PyTorch is CPU, so this restriction is usually problematic\n",
    "when we're on the GPU.\n",
    "\n",
    "For GPU accelerated computation, PyTorch tensors and networks provide a method\n",
    "called `to`. Example usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device(type='cpu')\n",
      "device(type='cuda', index=0)\n",
      "device(type='cpu')\n"
     ]
    }
   ],
   "source": [
    "# Create a 10 * 10 tensor with random values, drawn from a normal distribution\n",
    "a = torch.randn(10, 10)\n",
    "print(repr(a.device))\n",
    "\n",
    "# Send it to the GPU\n",
    "a = a.to(\"cuda\")\n",
    "print(repr(a.device))\n",
    "\n",
    "# Sending back to the CPU is also possible\n",
    "a = a.to(\"cpu\")\n",
    "print(repr(a.device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike tensors, the network does not provide a direct way to check its device\n",
    "(i.e., no `net.device`), so the following code uses the `weight` attribute to\n",
    "check the network's device. However, not all networks has the attribute we're\n",
    "using, and this workaround could also fail. This is just a demonstration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device(type='cpu')\n",
      "device(type='cuda', index=0)\n",
      "device(type='cpu')\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "# Create a network\n",
    "net = nn.Linear(10, 10)\n",
    "print(repr(net.weight.device))\n",
    "\n",
    "net = net.to(\"cuda\")\n",
    "print(repr(net.weight.device))\n",
    "\n",
    "net = net.to(\"cpu\")\n",
    "print(repr(net.weight.device))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a564f66d73a76c21eb3a3edb3dbdd1f865d9849a4084fedca96fb05e85623c37"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
